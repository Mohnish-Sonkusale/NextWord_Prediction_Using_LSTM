# Overview
Predicting word from the previous text written using LSTM in tensorflow.

Multi-layer Recurrent Neural Networks (LSTM, RNN) for character-level language models in Python using Tensorflow.

# INTRODUCTION:
How does the keyboard on your phone know what you would like to type next? Language prediction is a Natural Language Processing - NLP application concerned with predicting the text given in the preceding text. Auto-complete or suggested responses are popular types of language prediction. The first step towards language prediction is the selection of a language model.

Vanishing gradient descend is a problem faced by neural networks when we go for backpropagation. It has a huge effect and the weight update process is widely affected and the model became useless. So, we used LSTM which has a hidden state and a memory cell with three gates that are forgotten, read, and input gate.

# Requirements
Tensorflow 1.0 or greater

# WorkFlow
Data collection

Data preprocessing

Removal of unwanted words

Training Set

Building Text Matrices

Encoding Sequences

Input and output sequences

Building the model

RNN (Recurrent Neural Networks)

LSTM (Long Short Term Memory Cell)

Fitting the model

Testing the code
